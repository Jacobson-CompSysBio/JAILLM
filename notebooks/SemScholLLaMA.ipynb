{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-10 13:20:00,363] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import sys, os\n",
    "import torch \n",
    "import numpy as np\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from transformers import (pipeline,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorWithPadding,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom imports\n",
    "from utils.GetLowestGPU import GetLowestGPU\n",
    "\n",
    "device = GetLowestGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c107fb8b38454cdda72e4af4008fe282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "dataset_path = \"allenai/peS2o\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# load tokenizer and model\n",
    "pipeline = pipeline('text-generation', \n",
    "                    model=model_path,\n",
    "                    model_kwargs={'torch_dtype': torch.bfloat16},\n",
    "                    device_map = 'auto',\n",
    "                    \n",
    "                    )\n",
    "\n",
    "pipeline.model = get_peft_model(pipeline.model, peft_config)\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "pipeline.model.generation_config.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "\n",
    "pipeline.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['added', 'created', 'id', 'source', 'text', 'version'],\n",
       "        n_shards: 20\n",
       "    })\n",
       "    validation: IterableDataset({\n",
       "        features: ['added', 'created', 'id', 'source', 'text', 'version'],\n",
       "        n_shards: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_dataset(dataset_path, \"v2\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "# check format of data\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def preprocess_data(examples):\n",
    "    tokenized_data = pipeline.tokenizer(text=examples['text'],\n",
    "                               padding='max_length', \n",
    "                               truncation=True, \n",
    "                               max_length=128)\n",
    "    \n",
    "    labels = tokenized_data['input_ids'].copy()\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i][-1] != pipeline.tokenizer.pad_token_id:\n",
    "            labels[i] = labels[i][1:] + [pipeline.tokenizer.pad_token_id]\n",
    "        else:\n",
    "            labels[i] = labels[i][1:] + [-100]\n",
    "\n",
    "    labels = [[-100 if x == pipeline.tokenizer.pad_token_id else x for x in y] for y in labels]\n",
    "    tokenized_data['labels'] = labels\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 20\n",
       "    })\n",
       "    validation: IterableDataset({\n",
       "        features: Unknown,\n",
       "        n_shards: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add special tokens to tokenizer\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "pipeline.model.resize_token_embeddings(len(pipeline.tokenizer))\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess_data,\n",
    "                                    batched=True,\n",
    "                                    remove_columns=raw_dataset['train'].column_names,)\n",
    "tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "# check tokenized dataset output\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=pipeline.tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator,\n",
    "                              num_workers=20)\n",
    "\n",
    "val_dataloader = DataLoader(tokenized_dataset['validation'],\n",
    "                            batch_size=8,\n",
    "                            collate_fn=data_collator,\n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 512]),\n",
       " 'attention_mask': torch.Size([8, 512]),\n",
       " 'labels': torch.Size([8, 512])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect sample batch\n",
    "batch = next(iter(train_dataloader))\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.4993, grad_fn=<ToCopyBackward0>) torch.Size([8, 512, 128256])\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline.model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': 'network biology is a new interdisciplinary field of science that studies biological systems using methods formerly used in computer science, applied mathematics, control theory, and physics. It has emerged in the past decade as a way of understanding and studying the complex interactions that take place within biological systems and the complex relationships between various biological systems. The goal of systems biology is to understand the complex interactions of biological systems in order to predict and control their behavior.\\nSystems biology is a new field of science that studies biological systems using methods formerly used in computer science, applied mathematics, control theory, and physics. It has emerged in the past decade as a way of understanding and studying the complex interactions that take place within biological systems and the complex relationships between various biological systems. The goal of systems biology is to understand the complex interactions of biological systems in order to predict and control their behavior.\\nSystems biology is a new field of science that studies biological systems using methods formerly used in computer science, applied mathematics, control theory, and physics. It has emerged in the past decade as a way of understanding and studying the complex interactions that take place within biological systems and the complex relationships between various biological systems. The goal of systems biology is to understand the complex interactions of biological systems in order to predict and control their behavior.\\nSystems biology is a new field'}]]\n"
     ]
    }
   ],
   "source": [
    "# run a test prediction\n",
    "messages = [\"network biology is\"]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "#     pipeline.model, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 1\n",
      "=====================\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b887ba46804e99accc1a6bb56bc74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000 of 10000; Printing Example Response...\n",
      "[[{'generated_text': 'Network biology is a rapidly developing area of science that is concerned with the study of complex biological systems. In this article, we will explore the basics of network biology and how it can be used to gain insights into biological systems. We will start by discussing the basics of network theory, including the concepts of nodes and edges, and how these can be used to represent biological systems. We will then move on to discuss the different types of networks that are used in network biology, such as protein-protein interaction'}]]\n",
      "Batch 2000 of 10000; Printing Example Response...\n",
      "[[{'generated_text': 'Network biology is the study of biological networks, such as protein-protein interaction networks, gene regulatory networks, and metabolic networks. These networks are complex and often contain many nodes and edges, making them difficult to analyze. One approach to studying these networks is to use a graph theory approach, which involves representing the network as a graph and then analyzing the graph using graph theory techniques. This approach has been used to study a variety of biological networks, including protein-protein interaction networks, gene regulatory networks, and'}]]\n",
      "Batch 3000 of 10000; Printing Example Response...\n",
      "[[{'generated_text': 'Network biology is a complex system that can be used to understand the dynamics of a system. Understanding the system can help us to better understand how diseases work and can help us to identify new treatments. A network biology approach can be used to study the dynamics of a system by using a model that includes the interactions between different components of the system. This approach can be used to understand how a system works and can help us to identify new treatments.\\n\\n    \\n## 30 Creative Scarf Storage &amp; Display'}]]\n",
      "Batch Loss: 11.3388\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     52\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/DGX01/Personal/krusepi/.venv/lib/python3.8/site-packages/torch/_tensor.py:990\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# text for example after training\n",
    "text = [\"Network biology is\"]\n",
    "\n",
    "# options\n",
    "num_batches = 10_000\n",
    "num_epochs = 1\n",
    "best_val_loss = np.inf\n",
    "checkpoint_path = '../checkpoints/checkpoint_{0}.pt'\n",
    "log_path = '../logs/log.csv'\n",
    "\n",
    "# init optimizer\n",
    "optimizer = AdamW(pipeline.model.parameters(), lr=1e-7)\n",
    "\n",
    "# init scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=num_epochs * num_batches,\n",
    ")\n",
    "\n",
    "with open(log_path, 'w') as f: \n",
    "    f.write(f'epoch,iter_num,train_loss\\n')\n",
    "\n",
    "# loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print(\"=====================\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(\"=====================\")\n",
    "\n",
    "    # set model to train mode\n",
    "    pipeline.model.train()\n",
    "\n",
    "    # initialize train loss, val loss\n",
    "    running_train_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    # loop through train data\n",
    "    print(\"Training...\")\n",
    "    i = 0\n",
    "    with tqdm(total=num_batches) as pbar:\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            # grab batch and map to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # forward pass\n",
    "            outputs = pipeline.model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "\n",
    "            print(f\"Batch Loss: {loss:.4f}\\r\", end=\"\")\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # accelerator.backward(loss)\n",
    "\n",
    "            # update optimizer, scheduler\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "\n",
    "                # # save model checkpoint\n",
    "                # checkpoint = {\n",
    "                #     'model': pipeline.model.state_dict(),\n",
    "                #     'optimizer': optimizer.state_dict(),\n",
    "                #     'epoch': epoch,\n",
    "                #     'iter_num': i,``\n",
    "                #     'best_val_loss': best_val_loss,\n",
    "                # }\n",
    "                # torch.save(checkpoint, checkpoint_path.format(i))\n",
    "\n",
    "                # print example output\n",
    "                print(f\"Batch {i} of {num_batches}; Printing Example Response...\")\n",
    "                print(pipeline(text, max_length=100, truncation=True))\n",
    "\n",
    "            # write to log\n",
    "            with open(log_path, 'a') as f: \n",
    "                f.write(f'{epoch},{i},{loss}\\n')\n",
    "            \n",
    "            if i == num_batches:\n",
    "                break\n",
    "        \n",
    "    # set model to eval mode\n",
    "    pipeline.model.eval()\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline.model(**batch)\n",
    "            loss = outputs.loss\n",
    "            running_val_loss += loss.item()\n",
    "        \n",
    "    val_loss = running_val_loss / len(val_dataloader)\n",
    "\n",
    "    print(\"Epoch Complete; Printing example response...\")\n",
    "    print(pipeline(text, max_length=100, truncation=True))\n",
    "\n",
    "    train_loss = running_train_loss / len(train_dataloader)\n",
    "    print(f\"Avg. Train Loss: {train_loss:.4f}, Avg. Val Loss: {val_loss:.4f}\")\n",
    "    # print(\"Evaluation metrics:\", metric.compute())\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'generated_text': 'network biology is the study of biological systems using the tools of graph theory and combinatorics. It has been applied to many problems in molecular biology and genetics, such as the identification of functional modules in protein interaction networks, the analysis of gene expression data, and the prediction of drug targets. In this talk, we will introduce some basic concepts and techniques in network biology, and discuss some recent results on the analysis of protein interaction networks.\\n\\n• #### 2015/03/04 (Wed) 16:30 - 18:00\\n\\nYoshihiro Abe (Kyoto University)\\n\\n### A note on the asymptotic behavior of the number of self-avoiding walks\\n\\nWe consider the number $c_n$ of self-avoiding walks of length $n$ on the square lattice, and show that $c_n = \\\\exp(\\\\alpha n + o(n))$ for some $\\\\alpha > 0$. This is a short note of the proof of the well-known result.\\n\\n• #### 2015/02/25 (Wed) 16:30 - 18:00\\n\\nYoshihiro Abe (Kyoto University)\\n\\n### A note on the asymptotic behavior of the number of self-avoiding walks\\n\\nWe consider the number $c_n$ of self-'}]]\n"
     ]
    }
   ],
   "source": [
    "# run a test prediction\n",
    "messages = [\"network biology is\"]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
