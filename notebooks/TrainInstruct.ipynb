{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import sys, os\n",
    "import torch \n",
    "import numpy as np\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from transformers import (pipeline,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          DataCollatorWithPadding,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from importlib import reload\n",
    "from functools import partial\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom imports\n",
    "from utils.GetLowestGPU import GetLowestGPU\n",
    "from utils.GetFileNames import get_file_names\n",
    "import utils.preprocessing as pp\n",
    "\n",
    "device = GetLowestGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a27e8806518432fbb9abbd909c46f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# options\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "dataset_path = \"ruslanmv/ai-medical-chatbot\" #test dataset\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# load tokenizer and model\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_path,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "pipeline.model = get_peft_model(pipeline.model, peft_config)\n",
    "\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "pipeline.tokenizer.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "pipeline.model.generation_config.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "\n",
    "# pipeline.model, pipeline.tokenizer = setup_chat_format(pipeline.model, pipeline.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "pipeline.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Description', 'Patient', 'Doctor'],\n",
       "        num_rows: 4500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Description', 'Patient', 'Doctor'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_dataset(dataset_path, split = \"train[:5000]\")\n",
    "\n",
    "# check format of data\n",
    "raw_dataset = raw_dataset.train_test_split(test_size=0.1)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291cce81eadf49eab0d8c29c13c7db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fa4398e0704491ba00a665c5d11cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Description', 'Patient', 'Doctor', 'input', 'output'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format chat dataset\n",
    "reload(pp)\n",
    "format_chat = partial(pp.format_chat, input_col=\"Patient\", output_col=\"Doctor\", pipeline_name=pipeline)\n",
    "chat_dataset = raw_dataset.map(format_chat)\n",
    "chat_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c712613668a426998765313cee716e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e7dd15498c45d592cae7bb473df3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize dataset\n",
    "reload(pp)\n",
    "tokenize_function = partial(pp.tokenize_data, pipeline_name=pipeline, type=\"chat\")\n",
    "tokenized_dataset = chat_dataset.map(tokenize_function, \n",
    "                                     batched=True,\n",
    "                                     remove_columns=chat_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intputs: [128000, 128000, 128006, 882, 128007, 271, 9906, 10896, 38868, 1097]\n",
      "labels: [128000, 128000, 128006, 78191, 128007, 271, 9906, 13, 358, 4024]\n"
     ]
    }
   ],
   "source": [
    "print(\"intputs:\", tokenized_dataset['test'][0]['input_ids'][:10])\n",
    "print(\"labels:\", tokenized_dataset['test'][0]['labels'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=pipeline.tokenizer)\n",
    "\n",
    "# options\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                              batch_size=batch_size, \n",
    "                              collate_fn=data_collator)\n",
    "\n",
    "val_dataloader = DataLoader(tokenized_dataset['test'],\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 1024]),\n",
       " 'attention_mask': torch.Size([8, 1024]),\n",
       " 'labels': torch.Size([8, 1024])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect sample batch\n",
    "batch = next(iter(train_dataloader))\n",
    "{key: val.shape for key, val in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.7123, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline.model(input_ids=batch['input_ids'],\n",
    "                         attention_mask=batch['attention_mask'],\n",
    "                         labels=batch['labels'])\n",
    "print(outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'you are a helpful medical chatbot'}, {'role': 'user', 'content': 'I have a headache. What should I do?'}, {'role': 'assistant', 'content': \"Sorry to hear that you're experiencing a headache! As a helpful medical chatbot, I'd be happy to guide you through some steps to help alleviate your discomfort.\\n\\nHere are some suggestions:\\n\\n1. **Stay hydrated**: Dehydration is a common cause of headaches. Drink plenty of water or other fluids to help replenish your body's water\"}]\n"
     ]
    }
   ],
   "source": [
    "# test pre training\n",
    "text = [{'role': 'system', 'content': 'you are a helpful medical chatbot'},\n",
    "        {'role': 'user', 'content': 'I have a headache. What should I do?'}]\n",
    "print(pipeline(text, max_length=100, truncation=True)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 1\n",
      "=====================\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73649216bf9941abbb978d0210f62bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss: 13.0315\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# backward pass\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/_tensor.py:990\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# options\n",
    "optimizer = AdamW(pipeline.model.parameters(), lr=1e-5)\n",
    "num_epochs = 10\n",
    "num_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "# test after training\n",
    "text = [{'role': 'system', 'content': 'You are a helpful medical chatbot'},\n",
    "        {'role': 'user', 'content': 'I have a headache. What should I do?'}]\n",
    "\n",
    "# loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(\"=====================\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(\"=====================\")\n",
    "\n",
    "    # set model to train mode\n",
    "    pipeline.model.train()\n",
    "\n",
    "    # initialize train loss, val loss\n",
    "    running_train_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    # loop through train data\n",
    "    print(\"Training...\")\n",
    "    i = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # grab batch and map to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # forward pass\n",
    "        outputs = pipeline.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        print(f\"batch loss: {loss:.4f}\\r\", end=\"\")\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "        # if i % 5 == 0:\n",
    "        #     print(pipeline(text, max_length=100, truncation=True)[0]['generated_text'])\n",
    "            \n",
    "    # set model to eval mode\n",
    "    pipeline.model.eval()\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = pipeline.model(**batch)\n",
    "            loss = outputs.loss\n",
    "            running_val_loss += loss.item()\n",
    "        \n",
    "    val_loss = running_val_loss / len(val_dataloader)\n",
    "\n",
    "    print(\"Printing example response...\")\n",
    "    print(pipeline(text, max_length=100, truncation=True)[0]['generated_text'])\n",
    "\n",
    "    train_loss = running_train_loss / len(train_dataloader)\n",
    "    print(f\"Avg. Train Loss: {train_loss:.4f}, Avg. Val Loss: {val_loss:.4f}\")\n",
    "    # print(\"Evaluation metrics:\", metric.compute())\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful medical chatbot'}, {'role': 'user', 'content': 'I have a migraine. What should I do?'}, {'role': 'assistant', 'content': \"Sorry to hear that you're experiencing a migraine! As a helpful medical chatbot, I'm here to guide you through some steps to help alleviate your symptoms.\\n\\n**Immediate Relief:**\\n\\n1. **Stay hydrated**: Drink plenty of water to help your body replenish fluids and electrolytes. Dehydration is a common migraine trigger.\\n2. **Rest in a quiet, dark room**: This can help reduce visual and auditory stimuli that might exacerbate your migraine.\\n3. **Avoid triggers**: Identify and avoid potential triggers, such as bright lights, loud noises, or strong smells, that might have triggered your migraine.\\n\\n**Pain Relief:**\\n\\n1. **Over-the-counter pain relievers**: Try taking an over-the-counter pain reliever like acetaminophen (Tylenol), ibuprofen (Advil, Motrin), or aspirin. Always follow the recommended dosage and consult with your doctor if you have any concerns.\\n2. **Triptans**: If your migraines are frequent or severe, your doctor may have prescribed triptans, which can help constrict blood vessels and relieve pain. Always follow the recommended dosage and consult with your doctor before taking any medication.\\n\\n**Other Remedies:**\\n\\n1. **Cold or warm compresses**: Apply a cold or warm compress to the forehead, neck, or shoulders to help relax tense muscles and improve blood flow.\\n2. **Aromatherapy**: Essential oils like lavender, peppermint, and eucalyptus may help alleviate migraine symptoms. Try inhaling these oils or applying them topically (diluted with a carrier oil).\\n3. **Herbal teas**: Herbal teas like feverfew, ginger, and willow bark may help reduce inflammation and alleviate pain.\\n\\n**Prevention:**\\n\\n1. **Keep a migraine diary**: Tracking your migraines can help you identify patterns and triggers, allowing you to develop a personalized prevention plan.\\n2. **Maintain a consistent sleep schedule**: Irregular sleep patterns can trigger migraines. Aim for 7-8 hours of sleep each night.\\n3. **Stay consistent with medication**: If you're taking preventive medication, be sure to take it as directed to help reduce the frequency and severity of your migraines.\\n\\nRemember, if your migraines are severe, frequent, or accompanied by other concerning symptoms, consult with your healthcare provider for personalized guidance and treatment.\\n\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "# test after training\n",
    "text = [{'role': 'system', 'content': 'You are a helpful medical chatbot'},\n",
    "        {'role': 'user', 'content': 'I have a migraine. What should I do?'}]\n",
    "print(pipeline(text, max_length=512, truncation=True)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
