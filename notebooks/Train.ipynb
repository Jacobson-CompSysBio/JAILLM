{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import sys\n",
    "import torch \n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from transformers import (pipeline,\n",
    "                          DataCollatorWithPadding,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom modules\n",
    "import utils.preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "dataset_path = \"allenai/peS2o\"\n",
    "\n",
    "# for distributed training\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# for PEFT\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# load tokenizer and model\n",
    "pipeline = pipeline('text-generation', \n",
    "                    model=model_path,\n",
    "                    model_kwargs={'torch_dtype': torch.bfloat16},\n",
    "                    device_map = accelerator.device\n",
    "                    )\n",
    "\n",
    "pipeline.model = get_peft_model(pipeline.model, peft_config)\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "pipeline.tokenizer.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "pipeline.model.generation_config.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "\n",
    "pipeline.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['added', 'created', 'id', 'source', 'text', 'version'],\n",
       "        n_shards: 20\n",
       "    })\n",
       "    validation: IterableDataset({\n",
       "        features: ['added', 'created', 'id', 'source', 'text', 'version'],\n",
       "        n_shards: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_dataset(dataset_path, \"v2\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "# check format of data\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    n_shards: 20\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(pp)\n",
    "# add special tokens to tokenizer\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "pipeline.model.resize_token_embeddings(len(pipeline.tokenizer))\n",
    "\n",
    "tokenize_fn = partial(pp.tokenize_data, \n",
    "                      type = \"nextchar\",\n",
    "                      pipeline_name = pipeline,\n",
    "                      max_length = 100)\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_fn,\n",
    "                                    batched=True,\n",
    "                                    remove_columns=raw_dataset['train'].column_names,)\n",
    "tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "# check tokenized dataset output\n",
    "tokenized_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=pipeline.tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator,\n",
    "                              num_workers=20)\n",
    "\n",
    "val_dataloader = DataLoader(tokenized_dataset['validation'],\n",
    "                            batch_size=8,\n",
    "                            collate_fn=data_collator,\n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 100]), 'attention_mask': torch.Size([8, 100])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect sample batch\n",
    "batch = next(iter(train_dataloader))\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1162, device='cuda:0', grad_fn=<NllLossBackward0>) torch.Size([8, 100, 128256])\n"
     ]
    }
   ],
   "source": [
    "outputs = pipeline.model(batch['input_ids'].to(accelerator.device), labels=batch['input_ids'].to(accelerator.device), attention_mask=batch['attention_mask'].to(accelerator.device))\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Systems biology of cancer stem cells\n",
      "The cancer stem cell (CSC) hypothesis posits that a subset of cancer cells possesses stem cell-like properties and is responsible for tumour initiation, maintenance and relapse. This hypothesis has been the subject of intense scrutiny over the past decade, and has been the focus of numerous reviews. Here, we provide an overview of the evidence supporting the CSC hypothesis, including the identification of putative CSC markers, the development of CSC models, and the characterization of CSCs in various\n"
     ]
    }
   ],
   "source": [
    "# run a test prediction\n",
    "text = [\"Systems biology\"]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    text,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "print(outputs[0][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 2\n",
      "=====================\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9b03a979d8428a8c1564bd7b4a9cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# options\n",
    "num_batches = 1_000\n",
    "num_epochs = 10\n",
    "best_val_loss = np.inf\n",
    "checkpoint_path = '../checkpoints/checkpoint_{0}.pt'\n",
    "log_path = '../logs/log.csv'\n",
    "\n",
    "# init optimizer\n",
    "optimizer = AdamW(pipeline.model.parameters(), lr=1e-5)\n",
    "\n",
    "# init scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=num_epochs * num_batches,\n",
    ")\n",
    "\n",
    "pipeline.model, optimizer, train_dataloader, val_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    pipeline.model, optimizer, train_dataloader, val_dataloader, lr_scheduler)\n",
    "\n",
    "with open(log_path, 'w') as f: \n",
    "    f.write(f'epoch,iter_num,train_loss,val_loss\\n')\n",
    "\n",
    "# loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "\n",
    "    running_train_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    print(\"=====================\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(\"=====================\")\n",
    "\n",
    "    # loop through train data\n",
    "    print(\"Training...\")\n",
    "    with tqdm(total=num_batches) as pbar:\n",
    "        for i, (train_batch, val_batch) in enumerate(zip(train_dataloader, val_dataloader)):\n",
    "            \n",
    "            ## training\n",
    "            # set model to train mode\n",
    "            pipeline.model.train()\n",
    "\n",
    "            # grab batch and map to device\n",
    "            train_batch = {k: v.to(accelerator.device) for k, v in train_batch.items()}\n",
    "\n",
    "            # forward pass\n",
    "            outputs = pipeline.model(train_batch['input_ids'], \n",
    "                                     labels=train_batch['input_ids'],\n",
    "                                     attention_mask=train_batch['attention_mask'])\n",
    "            train_loss = outputs.loss\n",
    "\n",
    "            running_train_loss += train_loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            # train_loss.backward()\n",
    "            accelerator.backward(train_loss)\n",
    "\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(pipeline.model.parameters(), 1.0)\n",
    "\n",
    "            # update optimizer, scheduler\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ## validation\n",
    "            # set model to eval mode\n",
    "            pipeline.model.eval()\n",
    "            # loop through val data\n",
    "            val_batch = {k: v.to(accelerator.device) for k, v in val_batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = pipeline.model(val_batch['input_ids'], \n",
    "                                         labels=val_batch['input_ids'],\n",
    "                                         attention_mask=val_batch['attention_mask'])\n",
    "                val_loss = outputs.loss\n",
    "                running_val_loss += val_loss.item()\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "            \n",
    "            print(f\"Train Batch Loss: {train_loss:.4f} | Val Batch Loss: {val_loss:.4f} | Best Val. Loss: {best_val_loss:.4f}\\r\", end=\"\")\n",
    "\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # write to log\n",
    "            with open(log_path, 'a') as f: \n",
    "                f.write(f'{epoch},{i},{train_loss},{val_loss}\\n')\n",
    "            \n",
    "            if i == num_batches:\n",
    "                print(f\"Reached {num_batches} batches; starting next epoch...\")\n",
    "                \n",
    "                # break out of batching loop\n",
    "                break\n",
    "    \n",
    "    train_loss = running_train_loss / num_batches\n",
    "    val_loss = running_val_loss / num_batches\n",
    "    train_loss = running_train_loss / num_batches\n",
    "    print(f\"Avg. Train Loss: {train_loss:.4f}, Avg. Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# print example output\n",
    "print(f\"Training Complete; Printing Example Response...\")\n",
    "print(pipeline(text,\n",
    "                max_new_tokens=256,\n",
    "                eos_token_id=terminators,\n",
    "                no_repeat_ngram_size=3,       \n",
    "                do_sample=True, \n",
    "                top_k=100, \n",
    "                top_p=0.9,\n",
    "                temperature=0.6)[0][0]['generated_text'])\n",
    "    \n",
    "\n",
    "print(f\"Saving model checkpoint to {checkpoint_path.format(i)}\")\n",
    "# save model checkpoint\n",
    "checkpoint = {'model': pipeline.model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'iter_num': i,\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }\n",
    "torch.save(checkpoint, checkpoint_path.format(i))\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Systems biology is the study of biological systems.\n",
      "\n",
      "Systems biology, a new approach to the study and understanding of biological processes, is emerging as a powerful tool for investigating the dynamic properties of biological networks. The field of systems biology is based on the integration of high-throughput data with computational analysis, which enables the development of models that can be used to make predictions about the behavior of biological entities. The goal of systems biologists is to understand the complex interactions between genes, proteins, and other molecules that drive the behavior and function of cells. This approach is particularly useful in understanding how cells respond to external stimuli and how diseases develop. Systems biology has the potential to revolutionize the way we study and understand biological processes. By integrating data from multiple sources, systems biologist can gain a more complete picture of the underlying mechanisms that drive biological behavior. This information can then be used for a wide range of applications, including the development and testing of new therapies and the design of new technologies. Systems biologists are also able to use their knowledge of biological pathways to develop models that simulate the behavior or response of cells to different conditions. This can be useful in predicting the outcome of therapeutic interventions or in understanding the effects of environmental changes on biological processes\n"
     ]
    }
   ],
   "source": [
    "# run a test prediction\n",
    "outputs = pipeline(\n",
    "    text,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    no_repeat_ngram_size=3,       \n",
    "    do_sample=True, \n",
    "    top_k=100, \n",
    "    top_p=0.9,\n",
    "    temperature=0.6\n",
    ")\n",
    "print(outputs[0][0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
