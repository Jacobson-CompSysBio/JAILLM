{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import sys, os, glob\n",
    "import torch \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from transformers import (pipeline,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          DataCollatorWithPadding,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom imports\n",
    "from utils.GetLowestGPU import GetLowestGPU\n",
    "import utils.prepr`ocessing as pp\n",
    "\n",
    "device = GetLowestGPU()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d3d226b88f4f5d9d803b5a7bed9037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# load tokenizer and model\n",
    "pipeline = pipeline('text-generation', \n",
    "                    model=model_path,\n",
    "                    model_kwargs={'torch_dtype': torch.bfloat16},\n",
    "                    device_map = 'auto'\n",
    "                    )\n",
    "\n",
    "pipeline.model = get_peft_model(pipeline.model, peft_config)\n",
    "pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "pipeline.model.generation_config.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "\n",
    "pipeline.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/networks/DREAM4/DREAM4_in-silico_challenge/Size_10/DREAM4_gold_standards/'\n",
    "graph_names = glob.glob(dataset_path + \"*.tsv\")\n",
    "graph_names = [os.path.basename(x) for x in graph_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insilico_size10_4_goldstandard.tsv',\n",
       " 'insilico_size10_5_goldstandard.tsv',\n",
       " 'insilico_size10_1_goldstandard.tsv',\n",
       " 'insilico_size10_3_goldstandard.tsv',\n",
       " 'insilico_size10_2_goldstandard.tsv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert .txt Files to NetworkX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_nx(filename, graph_type = nx.Graph):\n",
    "    graph = nx.read_edgelist(filename,\n",
    "                             create_using=graph_type,\n",
    "                             nodetype=str,\n",
    "                             data=(('weight', float),))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64deca7bed39437391add90e050a2593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make empty list for graphs\n",
    "graphs = []\n",
    "\n",
    "for name in tqdm(graph_names):\n",
    "    # get filename\n",
    "    filename = dataset_path + name\n",
    "\n",
    "    # load graph\n",
    "    graph = txt_to_nx(filename)\n",
    "    \n",
    "    # append to list\n",
    "    graphs.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f64bc90ef70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def network_to_text(G):\n",
    "\n",
    "    nodes = []\n",
    "    for x in G.nodes():\n",
    "        nodes.append(x)\n",
    "\n",
    "    # collect edges and weights\n",
    "    edges = []\n",
    "    for u,v in G.edges():\n",
    "        edges.append(\"(\"+str(u)+\",\"+str(v)+\") with weight \" + str(G.get_edge_data(u,v)['weight']))\n",
    "\n",
    "    return(nodes, edges)\n",
    "\n",
    "def format_chat(nodes, edges, question):\n",
    "    row_json = [{'role': 'user',\n",
    "                 'content': 'In and undirected weighted graph, (i,j) means that node i and node j are connected with an undirected, weighted edge. The nodes are: {} and the edges are: {}\\n Is there a cycle in this graph?'},\n",
    "                {'role': 'assistant', 'content': 'yes'}]\n",
    "    row[\"text\"] = pipeline.tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    tokenized_data = pipeline.tokenizer(text=examples['text'],\n",
    "                               padding='max_length', \n",
    "                               truncation=True, \n",
    "                               max_length=1024)\n",
    "    \n",
    "    labels = tokenized_data['input_ids'].copy()\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if labels[i][-1] != pipeline.tokenizer.pad_token_id:\n",
    "            labels[i] = labels[i][1:] + [pipeline.tokenizer.pad_token_id]\n",
    "        else:\n",
    "            labels[i] = labels[i][1:] + [-100]\n",
    "\n",
    "    labels = [[-100 if x == pipeline.tokenizer.pad_token_id else x for x in y] for y in labels]\n",
    "    tokenized_data['labels'] = labels\n",
    "    \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# instantiate data collator\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(tokenizer\u001b[38;5;241m=\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[0;32m----> 4\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtokenized_dataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m                               batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[1;32m      6\u001b[0m                               collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m      7\u001b[0m                               num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      9\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(tokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m                             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     11\u001b[0m                             collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     12\u001b[0m                             num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# instantiate data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=pipeline.tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                              batch_size=8, \n",
    "                              collate_fn=data_collator,\n",
    "                              num_workers=20)\n",
    "\n",
    "val_dataloader = DataLoader(tokenized_dataset['validation'],\n",
    "                            batch_size=8,\n",
    "                            collate_fn=data_collator,\n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect sample batch\n",
    "batch = next(iter(train_dataloader))\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pipeline.model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "num_batches = 10_000\n",
    "num_epochs = 1\n",
    "best_val_loss = np.inf\n",
    "checkpoint_path = '../checkpoints/checkpoint_{0}.pt'\n",
    "log_path = '../logs/log.csv'\n",
    "\n",
    "# init optimizer\n",
    "optimizer = AdamW(pipeline.model.parameters(), lr=1e-5)\n",
    "\n",
    "# init scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1000,\n",
    "    num_training_steps=num_epochs * num_batches,\n",
    ")\n",
    "\n",
    "with open(log_path, 'w') as f: \n",
    "    f.write(f'epoch,iter_num,train_loss,val_loss\\n')\n",
    "\n",
    "# loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print(\"=====================\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(\"=====================\")\n",
    "\n",
    "    # initialize train loss, val loss\n",
    "    running_train_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    # loop through train data\n",
    "    print(\"Training...\")\n",
    "    i = 0\n",
    "    with tqdm(total=num_batches) as pbar:\n",
    "        for train_batch, val_batch in zip(train_dataloader, val_dataloader):\n",
    "            \n",
    "            ## training\n",
    "            # set model to train mode\n",
    "            pipeline.model.train()\n",
    "\n",
    "            # grab batch and map to device\n",
    "            train_batch = {k: v.to(device) for k, v in train_batch.items()}\n",
    "\n",
    "            # forward pass\n",
    "            outputs = pipeline.model(**batch)\n",
    "            train_loss = outputs.loss\n",
    "\n",
    "            running_train_loss += train_loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            train_loss.backward()\n",
    "            # accelerator.backward(loss)\n",
    "\n",
    "            # clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(pipeline.model.parameters(), 1.0)\n",
    "\n",
    "            # update optimizer, scheduler\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ## validation\n",
    "            # set model to eval mode\n",
    "            pipeline.model.eval()\n",
    "            # loop through val data\n",
    "            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = pipeline.model(**batch)\n",
    "                val_loss = outputs.loss\n",
    "                running_val_loss += val_loss.item()\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "            \n",
    "            print(f\"Train Batch Loss: {train_loss:.4f} | Val Batch Loss: {val_loss:.4f} | Best Val. Loss: {best_val_loss:.4f}\\r\", end=\"\")\n",
    "\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "            if i % 1000 == 0:\n",
    "\n",
    "                # # save model checkpoint\n",
    "                # checkpoint = {\n",
    "                #     'model': pipeline.model.state_dict(),\n",
    "                #     'optimizer': optimizer.state_dict(),\n",
    "                #     'epoch': epoch,\n",
    "                #     'iter_num': i,``\n",
    "                #     'best_val_loss': best_val_loss,\n",
    "                # }\n",
    "                # torch.save(checkpoint, checkpoint_path.format(i))\n",
    "                \n",
    "            # write to log\n",
    "            with open(log_path, 'a') as f: \n",
    "                f.write(f'{epoch},{i},{train_loss},{val_loss}\\n')\n",
    "            \n",
    "            if i == num_batches:\n",
    "                print(f\"Reached {num_batches} batches; breaking...\")\n",
    "                break\n",
    "    \n",
    "    train_loss = running_train_loss / num_batches\n",
    "    val_loss = running_val_loss / num_batches\n",
    "\n",
    "    print(\"Epoch Complete; Printing example response...\")\n",
    "    print(pipeline(text, max_length=100, truncation=True))\n",
    "\n",
    "    train_loss = running_train_loss / len(train_dataloader)\n",
    "    print(f\"Avg. Train Loss: {train_loss:.4f}, Avg. Val Loss: {val_loss:.4f}\")\n",
    "    # print(\"Evaluation metrics:\", metric.compute())\n",
    "\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Predictions on Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define questions\n",
    "connectivity = \"Is there a path between node {node1} and node {node2} in this undirected graph?\"\n",
    "\n",
    "cycle = \"Is there a cycle in this undirected graph?\"\n",
    "\n",
    "top_sort = \"Perform a topological sort of this undirected graph.\"\n",
    "\n",
    "shortest_path = \"What path between {node1} and {node2} minimizes the weights of their constituent edges?\"\n",
    "\n",
    "max_flow = \"What is the maximum amount of flow that can travel through this undirected graph?\"\n",
    "\n",
    "bipartite_graph_matching = \"Find the maximum bipartite matching of this graph.\"\n",
    "\n",
    "hamilton_path = \"Find all possible paths in this undirected graph that visit each vertex exactly once.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get example graph for asking questions\n",
    "ex = graphs[0]\n",
    "nodes, edges = network_to_text(ex)\n",
    "node_str = \", \".join(nodes[1:20])\n",
    "edges_str = \", \".join(edges[1:20])\n",
    "\n",
    "# get question\n",
    "question = top_sort\n",
    "\n",
    "message = [{'role': 'system', 'content': 'You are an expert on topology.'},\n",
    "           {'role': 'user',\n",
    "            'content': 'In and undirected weighted graph, (i,j) means that node i '\\\n",
    "            +'and node j are connected with an undirected, weighted edge. '\\\n",
    "            +f'The nodes are: {node_str} '\\\n",
    "            +f'and the edges are: {edges_str}. '\\\n",
    "            +f'{question}'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an expert on topology.'}, {'role': 'user', 'content': 'In and undirected weighted graph, (i,j) means that node i and node j are connected with an undirected, weighted edge. The nodes are: G2, G3, G9, G4, G5, G6, G7, G8, G10 and the edges are: (G1,G4) with weight 0.0, (G1,G5) with weight 0.0, (G1,G7) with weight 0.0, (G1,G3) with weight 0.0, (G1,G6) with weight 0.0, (G1,G8) with weight 0.0, (G1,G9) with weight 0.0, (G1,G10) with weight 0.0, (G2,G3) with weight 0.0, (G2,G9) with weight 0.0, (G2,G4) with weight 0.0, (G2,G5) with weight 0.0, (G2,G6) with weight 0.0, (G2,G7) with weight 0.0, (G2,G8) with weight 0.0, (G2,G10) with weight 0.0, (G3,G4) with weight 0.0, (G3,G6) with weight 0.0, (G3,G8) with weight 0.0. Perform a topological sort of this undirected graph.'}, {'role': 'assistant', 'content': 'A topological sort of an undirected graph is not possible because it is not a directed acyclic graph (DAG). In a topological sort, we can\\'t have an edge from node A to node B and also from node B to node A. In this graph, all edges are undirected, so we can\\'t determine a specific direction for any edge.\\n\\nHowever, we can convert this undirected graph into a directed graph by replacing each undirected edge with two directed edges, one in each direction. This is called a \"bipartite\" graph. After converting the graph, we can perform a topological sort on the resulting directed graph.\\n\\nHere is the converted graph:\\n\\nDirected edges:\\n\\n* (G1, G4) with weight 0.0\\n* (G1, G5) with weight 0.0\\n* (G1, G7) with weight 0.0\\n* (G1, G3) with weight 0.0\\n* (G1, G6) with weight 0.0\\n* (G1, G8) with weight 0.0\\n* (G1, G9) with weight 0.0\\n* (G1, G10) with weight 0.0\\n* (G2, G3) with weight 0.0\\n* (G3, G2) with weight 0.0\\n* (G2, G9) with weight 0.0\\n* (G9, G2) with weight 0.0\\n* (G2, G4) with weight 0.0\\n* (G4, G2) with weight 0.0\\n* (G2, G5) with weight 0.0\\n* (G5, G2) with weight 0.0\\n* (G2, G6) with weight 0.0\\n* (G6, G2) with weight 0.0\\n* (G2, G7) with weight 0.0\\n* (G7, G2) with weight 0.0\\n* (G2, G8) with weight 0.0\\n* (G8, G2) with weight 0.0\\n* (G2, G10) with weight 0.0\\n* (G10, G2) with weight 0.0\\n* (G3, G4) with weight 0.0\\n* (G4, G3) with weight 0.0\\n* (G3, G6) with weight 0.0\\n* (G6, G3) with weight 0.0\\n* (G3, G8) with weight 0.0\\n* (G8, G3) with weight 0.0\\n\\nNow, we can perform a topological sort on this directed graph. The result is:\\n\\nG1, G2, G3, G4, G5, G6, G7, G8, G9, G10\\n\\nThis is a valid topological sort because the graph is a DAG (directed acyclic graph).'}]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline(message, max_new_tokens=2048, truncation=False)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
