{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cuda:0\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import sys, os\n",
    "import torch \n",
    "import numpy as np\n",
    "import evaluate\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from transformers import (pipeline,\n",
    "                          AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          DataCollatorWithPadding,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom imports\n",
    "from utils.GetLowestGPU import GetLowestGPU\n",
    "\n",
    "device = GetLowestGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130b58e697bc4571a04464ebcea95e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "dataset_path = \"ruslanmv/ai-medical-chatbot\" #test dataset\n",
    "\n",
    "# load tokenizer and model\n",
    "pipeline = pipeline('text-generation', \n",
    "                    model=model_path,\n",
    "                    model_kwargs={'torch_dtype': torch.bfloat16},\n",
    "                    device_map = 'auto'\n",
    "                    )\n",
    "\n",
    "model, tokenizer = pipeline.model, pipeline.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Description', 'Patient', 'Doctor'],\n",
       "        num_rows: 2312\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Description', 'Patient', 'Doctor'],\n",
       "        num_rows: 257\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_dataset(dataset_path, split = 'train[:1%]')\n",
    "\n",
    "# check format of data\n",
    "raw_dataset = raw_dataset.train_test_split(test_size=0.1)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "def format_chat(row):\n",
    "    row_json_inp = [{\"role\": \"user\", \"content\": row[\"Patient\"]}]\n",
    "    row_json_out = [{\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "    row[\"user\"] = tokenizer.apply_chat_template(row_json_inp, tokenize=False)\n",
    "    row[\"assistant\"] = tokenizer.apply_chat_template(row_json_out, tokenize=False)\n",
    "    return row\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    inp = examples[\"user\"]\n",
    "    out = examples[\"assistant\"]\n",
    "    tokenized_data = tokenizer(text=inp, \n",
    "                               text_target=out,\n",
    "                               padding='max_length', \n",
    "                               truncation=True, \n",
    "                               max_length=512)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c25d87dff4d4cd9aebcd72903a44189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e010650c944db782a468e81c5429e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Description': 'Q. Suffering from osteopenia and osteoarthritis. Taking AcuCal and FreeFlex. Is it safe to take SAM(S Adenosyl Methionine) to enhance mobility?',\n",
       " 'Patient': 'Dear Doctor, My mother is 66 years old. She is having Trigeminal Neuralgia and for which she is taking Lyrica 75mg, Tegritol CR 400 and Symbal 30. And other medication details as below: For Cholesetrol -> Storfib -10mg. For BP -> seloken xl 100mg and amlodac 5mg. For Urinary Incontinence -> tropan 5mg and cranpac. Her bone mineral density revealed to scores of Osteopenia, and also Osteoarthritis. She is taking AcuCal calcium tablet and FreeFlex Forte. But her mobility is still very much restricted. So I want to know whether she can take SAM (S Adenosyl Methionine), and is it safe? Also If she can take SAM, what will be the dosage and duration. I am asking whether she can take this as a supportive to enhance her mobility. She is a pure vegetarian and on fat free diet. Her weight is 70kg, height is 5.1 feet. Please give your suggestion.',\n",
       " 'Doctor': 'Hi, Welcome to icliniq and thanks for the query. She can take SAM or MSM (Any one of them). SAM is a safe medication. It is a cartilage protective drug and will help her improve some of the pain in osteoarthritis. For osteoarthritis: you have to remember that these medications like free flex, collagen peptides, SAM and MSM are like to help to nourish cartilage and provide some relief. They do not actually replenish the cartilage. I normally advise to take these cartilage supplements for a period of 3 months and gap of 6 months and again restart them. Just check the medication freeflex might contain MSM. Regarding osteopenia, I advise her to continue calcium and consult local orthopaedic surgeon if he can help her with some medications that can build up calcium. If her mobility is restricted, then better to get an x-ray and check the current stage of osteoarthritis and depending on it, we can suggest further treatment. Consult an orthopaedician online -->',\n",
       " 'user': '<|im_start|>user\\nDear Doctor, My mother is 66 years old. She is having Trigeminal Neuralgia and for which she is taking Lyrica 75mg, Tegritol CR 400 and Symbal 30. And other medication details as below: For Cholesetrol -> Storfib -10mg. For BP -> seloken xl 100mg and amlodac 5mg. For Urinary Incontinence -> tropan 5mg and cranpac. Her bone mineral density revealed to scores of Osteopenia, and also Osteoarthritis. She is taking AcuCal calcium tablet and FreeFlex Forte. But her mobility is still very much restricted. So I want to know whether she can take SAM (S Adenosyl Methionine), and is it safe? Also If she can take SAM, what will be the dosage and duration. I am asking whether she can take this as a supportive to enhance her mobility. She is a pure vegetarian and on fat free diet. Her weight is 70kg, height is 5.1 feet. Please give your suggestion.<|im_end|>\\n',\n",
       " 'assistant': '<|im_start|>assistant\\nHi, Welcome to icliniq and thanks for the query. She can take SAM or MSM (Any one of them). SAM is a safe medication. It is a cartilage protective drug and will help her improve some of the pain in osteoarthritis. For osteoarthritis: you have to remember that these medications like free flex, collagen peptides, SAM and MSM are like to help to nourish cartilage and provide some relief. They do not actually replenish the cartilage. I normally advise to take these cartilage supplements for a period of 3 months and gap of 6 months and again restart them. Just check the medication freeflex might contain MSM. Regarding osteopenia, I advise her to continue calcium and consult local orthopaedic surgeon if he can help her with some medications that can build up calcium. If her mobility is restricted, then better to get an x-ray and check the current stage of osteoarthritis and depending on it, we can suggest further treatment. Consult an orthopaedician online --><|im_end|>\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dataset = raw_dataset.map(format_chat)\n",
    "chat_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244e71af286c44758b13d01bd39f3fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d2c05f307b4aa3ad4785b9f77875f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2312\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 257\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add special tokens to tokenizer\n",
    "tokenized_dataset = chat_dataset.map(preprocess_data, \n",
    "                                    batched=True,\n",
    "                                    remove_columns=chat_dataset['train'].column_names)\n",
    "tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "# check tokenized dataset output\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# options\n",
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                              batch_size=batch_size, \n",
    "                              collate_fn=data_collator)\n",
    "\n",
    "val_dataloader = DataLoader(tokenized_dataset['test'],\n",
    "                            batch_size=batch_size,\n",
    "                            collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 512]),\n",
       " 'attention_mask': torch.Size([8, 512]),\n",
       " 'labels': torch.Size([8, 512])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect sample batch\n",
    "batch = next(iter(train_dataloader))\n",
    "{key: val.shape for key, val in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.7973, grad_fn=<ToCopyBackward0>) torch.Size([8, 512, 128258])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful medical chatbot'}, {'role': 'user', 'content': 'I have a headache. What should I do?'}, {'role': 'assistant', 'content': \"Sorry to hear that you're experiencing a headache! There are several steps you can take to help alleviate the discomfort. Here are some suggestions:\\n\\n1. **Stay hydrated**: Dehydration is a common cause of headaches. Drink a glass of water or other non-caffeinated fluid to see if that helps.\\n2. **Take a pain reliever**:\"}]\n"
     ]
    }
   ],
   "source": [
    "# test pre training\n",
    "text = [{'role': 'system', 'content': 'You are a helpful medical chatbot'},\n",
    "        {'role': 'user', 'content': 'I have a headache. What should I do?'}]\n",
    "print(pipeline(text, max_length=100, truncation=True)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n"
     ]
    }
   ],
   "source": [
    "# initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# and scheduler\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 1\n",
      "=====================\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e959cd5bef20494396165ed9f11606e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217ad6aafc494867baa8f831d001d8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Train Loss: 17.117546503931592, Avg. Val Loss: 14.68741343238137\n"
     ]
    }
   ],
   "source": [
    "# eval loop\n",
    "# loop through epochs\n",
    "for epoch in range(num_epochs):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(\"=====================\")\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(\"=====================\")\n",
    "\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # initialize train loss, val loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # loop through train data\n",
    "    print(\"Training...\")\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # grab batch and map to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # update scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    train_loss = train_loss / (len(train_dataloader) / batch_size)\n",
    "\n",
    "    # set to eval mode\n",
    "    model.eval()\n",
    "    print(\"Validating...\")\n",
    "    for batch in tqdm(val_dataloader):\n",
    "\n",
    "        # get batch\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        # get loss\n",
    "        loss = outputs.loss\n",
    "        val_loss += loss.item()\n",
    "\n",
    "\n",
    "    val_loss = val_loss / (len(val_dataloader) / batch_size)\n",
    "\n",
    "    print(f\"Avg. Train Loss: {train_loss}, Avg. Val Loss: {val_loss}\")\n",
    "    # print(\"Evaluation metrics:\", metric.compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful medical chatbot'}, {'role': 'user', 'content': 'I have a migraine. What should I do?'}, {'role': 'assistant', 'content': ''}]\n"
     ]
    }
   ],
   "source": [
    "# test after training\n",
    "text = [{'role': 'system', 'content': 'You are a helpful medical chatbot'},\n",
    "        {'role': 'user', 'content': 'I have a migraine. What should I do?'}]\n",
    "print(pipeline(text, max_length=100, truncation=True)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
